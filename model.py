from torch import nn
import torch
import matplotlib.pyplot as plt

#inpinp shapeï¼š[batch,seq_len,d_model]
class PositionEncoding(nn.Module):
    def __init__(self,d_model,max_seq_len=512):
        super().__init__()
        #shape:[max_sql_len,1]
        position=torch.arange(0,max_seq_len).unsqueeze(1)
        
        item=1/10000**(torch.arange(0,d_model,2)/d_model)

        tmp_pos=position*item

        pe=torch.zeros(max_seq_len,d_model)

        pe[:,0::2]=torch.sin(tmp_pos)
        pe[:,1::2]=torch.cos(tmp_pos)

        # plt.matshow(pe)
        # plt.show()

        pe=pe.unsqueeze(0)
        #å¢åŠ ä¸€ä¸ªç»´åº¦å˜ä¸ºï¼ˆbatch_size,max_seq_len,d_model)
        self.register_buffer("pe",pe,False)

    def forward(self,x):
        batch,seq_len,_=x.shape
        pe=self.pe
        return x+pe[:,:seq_len,:]
    
def attention(query,key,value,mask=None):
    d_model=key.shape[-1]
    #query,key,value shape:[batch,head,seq_len,d_k]

    att=torch.matmul(query,key.transpose(-2,-1))/torch.tensor(d_model).sqrt()


    if mask is not None:
        att=att.masked_fill(mask,-1e9)
        #åˆ†ä¸ºKeyPaddingMaskä¸SeqMask
        #keyPaddingMaskï¼šåºåˆ—ä½ç½®ä¸ºpaddingçš„ä½ç½®çš„attention scoreè®¾ç½®ä¸º-1e9
        #batchçš„åºåˆ—é•¿åº¦éœ€è¦ç»Ÿä¸€ï¼ˆä¸ºäº†æ–¹ä¾¿GPUè¿ç®—ï¼‰ï¼Œå¥å­åºåˆ—é•¿åº¦ä¸è¶³çš„å¥å­éœ€è¦paddingå¡«å……
        #SeqMaskï¼šæ˜¯decoderä¸­self-attentionçš„mask
        # ï¼šå…¶ä¸»è¦ä½œç”¨æ˜¯å±è”½æœªæ¥ï¼Œé˜²æ­¢æ¨¡å‹åœ¨è®­ç»ƒçš„æ—¶å€™å·çœ‹æœªæ¥ä¿¡æ¯

    att_score=torch.softmax(att,dim=-1)
    # att_scoreçš„shapeä¸º[batch_size,num_heads,max_seq_len_q,max_seq_len_k]
    out=torch.matmul(att_score,value)
    #valueçš„shapeä¸º[batch_size,num_heads,max_seq_len_v,d_k]
    #outçš„shapeä¸º[batch_size,num_heads,max_seq_len_q,d_k]
    return out

class MultiHeadAttention(nn.Module):
    def __init__(self,heads,d_model,dropout):
        super().__init__()
        assert d_model%heads==0
        self.heads=heads
        self.dropout=nn.Dropout(dropout)
        self.d_k=d_model//heads
        self.d_model=d_model
        self.q_linear=nn.Linear(d_model,d_model,bias=False)
        self.k_linear=nn.Linear(d_model,d_model,bias=False)
        self.v_linear=nn.Linear(d_model,d_model,bias=False)
        self.out_linear=nn.Linear(d_model,d_model,bias=False)

    def forward(self,q,k,v,mask=None):
        #[n,seq_len,d_model] -> [n,heads,seq_len,d_k]
        q=self.q_linear(q).view(q.shape[0],-1,self.heads,self.d_k).transpose(1,2)
        k=self.k_linear(k).view(k.shape[0],-1,self.heads,self.d_k).transpose(1,2)
        v=self.v_linear(v).view(v.shape[0],-1,self.heads,self.d_k).transpose(1,2)
        out=attention(q,k,v,mask).transpose(1,2).contiguous().view(q.shape[0],-1,self.d_model)
        #attention(q,k,v,mask)ä¼ å…¥xç»WkWqWvæ˜ å°„åçš„q,k,vï¼Œä¸keypaddingmaskï¼Œå¹¶è¿”å›attentionåçš„ç»“æœ
        #transpose(1,2).contiguous().view(q.shape[0],-1,self.d_model)
        #å…ˆæ¢ç»´åº¦ä½ç½®ä¸º([batch_size,seq_len,heads,d_k])å†è½¬æˆ([batch_size,seq_len,d_model])åˆå¹¶å¤šå¤´
        #æ‹¼æ¥åçš„ ğ»åªæ˜¯æŠŠä¸åŒ head çš„ä¿¡æ¯â€œå †åœ¨ä¸€èµ·â€ï¼Œè¿˜æ²¡æœ‰çœŸæ­£èåˆ
        #æ¯ä¸ª head çš„ä¿¡æ¯åœ¨æ‹¼æ¥åæ˜¯å›ºå®šé¡ºåºçš„ï¼Œæ¨¡å‹æ²¡æ³•è‡ªç”±æ··åˆ
        #ç»è¿‡åŠ ä¸Š ğ‘Šğ‘‚çš„çº¿æ€§æ˜ å°„ï¼Œæ¨¡å‹å¯ä»¥å­¦åˆ° å¤æ‚çš„è·¨å¤´äº¤äº’ï¼Œæå‡è¡¨è¾¾èƒ½åŠ›
        out=self.out_linear(out)#Wo
        #Woå°±æ˜¯å­¦ä¹ å¦‚ä½•åŠ æƒç»„åˆå„ä¸ª headçš„è¾“å‡º
        return self.dropout(out)

class Feedforward(nn.Module):
    def __init__(self,d_model,d_ff,dropout=0.1):
        super().__init__()
        self.ffn=nn.Sequential(
            #ä¼ å…¥çš„å‚æ•°ä¸ºmutiheadattentionçš„è¾“å‡º.c
            nn.Linear(d_model,d_ff,bias=False),
            #shape[batch_size,max_seq_len,d_model]->shape[batch_size,max_seq_len,d_ff]
            nn.ReLU(),
            nn.Linear(d_ff,d_model,bias=False),
            #shape[batch_size,max_seq_len,d_ff]->shape[batch_size,max_seq_len,d_model]
            nn.Dropout(dropout)
        )

    def forward(self,x):
        return self.ffn(x)
class EncoderLayer(nn.Module):
    def __init__(self,heads,d_model,d_ff,dropout=0.1):
        super().__init__()
        self.multi_heads_att=MultiHeadAttention(heads,d_model,dropout)
        self.ffn=Feedforward(d_model,d_ff,dropout)
        self.norms=nn.ModuleList([nn.LayerNorm(d_model)for i in range(2)])
        self.dropout=nn.Dropout(dropout)

    def forward(self,x,mask=None):
        multi_heads_att_out=self.multi_heads_att(x,x,x,mask)
        #è®¡ç®—å¤šå¤´æ³¨æ„åŠ›ï¼š
        multi_heads_att_out=self.norms[0](x+multi_heads_att_out)#æ®‹å·®+å±‚å½’ä¸€åŒ–
        ffn_out=self.ffn(multi_heads_att_out)#feedforwardNeuralNetworkå‰é¦ˆç¥ç»ç½‘ç»œ
        ffn_out=self.norms[1](multi_heads_att_out+ffn_out)
        #LayerNormå½’ä¸€åŒ–æ˜¯å¯¹æ¯ä¸€ä¸ªè¯tokenè¿›è¡ŒNormï¼ˆ0å‡å€¼ï¼Œ1æ–¹å·®ï¼‰
        out=self.dropout(ffn_out)
        return out

class Encoder(nn.Module):
    def __init__(self,vocab_size,d_model,pad_idx,heads,d_ff,n_layers,dropout=0.1,max_seq_len=512):

        super().__init__()
        self.embedding=nn.Embedding(vocab_size,d_model,pad_idx)
        #è¯åµŒå…¥Embedding:åœ¨è¡¨ä¸­å¯»æ‰¾å¥å­ä¸­å‡ºç°çš„è¯tokenç´¢å¼•
        #è¯åµŒå…¥ (Embedding): å°†å¥å­ä¸­çš„æ¯ä¸ª token ç´¢å¼•æ˜ å°„ä¸º d_model ç»´çš„å‘é‡è¡¨ç¤ºã€‚
        self.position_encode=PositionEncoding(d_model,max_seq_len)
        #ä½ç½®ç¼–ç PositionEncoding:ä¸ºå¥å­ä¸­çš„æ¯ä¸ªè¯æ·»åŠ ä¸€ä¸ªä½ç½®ç¼–ç å‘é‡ï¼Œ
        # ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿç†è§£å•è¯çš„ç›¸å¯¹ä½ç½®å…³ç³»ã€‚posä¸ºå¥å­ä¸­çš„ä½ç½®ç´¢å¼•ï¼Œiä¸ºd_modelçš„indexã€‚
        self.encoder_layers=nn.ModuleList([EncoderLayer(heads,d_model,d_ff,dropout)for i in range(n_layers)])
        #å®šä¹‰å¤šä¸ªEncoderLayer

    def forward(self,x,src_mask=None):
        embed_X=self.embedding(x)
        pos_embed_x=self.position_encode(embed_X)#æ·»åŠ ä½ç½®ç¼–ç 
        #æ·»åŠ å®Œä½ç½®ç¼–ç çš„pos_embedæ˜¯åŸå§‹è¾“å…¥x
        for layer in self.encoder_layers:
            pos_embed_x=layer(pos_embed_x,src_mask)
            #å°†ä¸Šä¸€å±‚è¾“å‡ºä¼ å…¥ä¸‹ä¸€å±‚ EncoderLayer
            # æ¯ä¸ª EncoderLayer å†…éƒ¨åŒ…å«:
            # 1. Multi-Head Attention (å¸¦æ®‹å·®å’Œ LayerNorm)
            # 2. å‰é¦ˆç½‘ç»œ Feedforward (å¸¦æ®‹å·®å’Œ LayerNorm)
        return pos_embed_x
        # è¾“å‡ºæ•´ä¸ª Encoder çš„è¡¨ç¤ºï¼Œå½¢çŠ¶ (batch_size, seq_len, d_model)
        # æ¯ä¸ª token éƒ½æœ‰ä¸€ä¸ª d_model ç»´çš„å‘é‡è¡¨ç¤ºï¼ŒåŒ…å«äº†ä¸Šä¸‹æ–‡ä¿¡æ¯


class DecoderLayer(nn.Module):
    def __init__(self, heads,d_model,d_ff,dropout=0.1):
        super().__init__()
        self.masked_att=MultiHeadAttention(heads,d_model,dropout)#å¸¦æ©ç çš„æ³¨æ„åŠ›
        self.att=MultiHeadAttention(heads,d_model,dropout)
        #cross attentionï¼ˆqï¼šç”±decoderçš„ä¸Šä¸€å±‚çš„masked attentionè¾“å…¥ï¼‰
        #kï¼Œvï¼šä»¥encoderçš„è¾“å‡ºä½œä¸ºè¾“å…¥
        self.ffn=Feedforward(d_model,d_ff,dropout)
        self.norms=nn.ModuleList([nn.LayerNorm(d_model)for i in range(3)])
        self.dropout=nn.Dropout(dropout)

    def forward(self,x,dst_mask,src_dst_mask,encode_kv):
        mask_att_out=self.masked_att(x,x,x,dst_mask)
        #decoderçš„ç¬¬ä¸€ä¸ªattï¼šMulti-Head masked attention
        mask_att_out=self.norms[0](x+mask_att_out)
        src_dst_att_out=self.att(mask_att_out,encode_kv,encode_kv,src_dst_mask)
        #decoderçš„ç¬¬äºŒä¸ªattï¼šMulti-Head crossed attention
        src_dst_att_out=self.norms[1](mask_att_out+src_dst_att_out)
        ffn_out=self.ffn(src_dst_att_out)
        ffn_out=self.norms[2](src_dst_att_out+ffn_out)
        out=self.dropout(ffn_out)
        return out


class Decoder(nn.Module):
    def __init__(self,vocab_size,d_model,pad_idx,heads,d_ff,n_layers,dropout=0.1,max_seq_len=512):
        super().__init__()
        self.embedding=nn.Embedding(vocab_size,d_model,pad_idx)
        self.position_encode=PositionEncoding(d_model,max_seq_len)
        self.decoder_layers=nn.ModuleList([DecoderLayer(heads,d_model,d_ff,dropout)for i in range(n_layers)])
    
    def forward(self,x,encode_kv,dst_mask=None,src_dst_mask=None):
        #decoderçš„åŸå§‹è¾“å…¥x
        dst_embedding=self.embedding(x)
        #å¯¹xè¿›è¡ŒEmbedding
        pos_embed_dst=self.position_encode(dst_embedding)
        #åŠ ä¸Šposä½ç½®ä¿¡æ¯
        for layer in self.decoder_layers:
            pos_embed_dst=layer(pos_embed_dst,dst_mask,src_dst_mask,encode_kv)
        return pos_embed_dst


class Transformer(nn.Module):
    def __init__(self,enc_vocab_size,dec_vocab_size,pad_idx,n_layers,heads,d_model,d_ff,dropout=0.1,max_seq_len=512):
        super().__init__()
        self.encoder=Encoder(enc_vocab_size,d_model,pad_idx,heads,d_ff,n_layers,dropout,max_seq_len)
        self.decoder=Decoder(dec_vocab_size,d_model,pad_idx,heads,d_ff,n_layers,dropout,max_seq_len)
        self.out_linear=nn.Linear(d_model,dec_vocab_size)
        self.pad_idx=pad_idx
    
    def generate_mask(self,query,key,is_triu_mask=False):
        #attention.shape=[batch,heads,seq_q,seq_k]
        #maskæ˜¯ä½œç”¨äºattentionä¸Šçš„ï¼Œæ‰€ä»¥å…¶å¼ é‡ç›®æ ‡ä¸ºï¼š[batch, 1, seq_len_q, seq_len_k]
        #query shape:[batch,seq_len_q,d_model]

        device=query.device
        batch,seq_q=query.shape[:2]
        _,seq_k=key.shape[:2]
        #ç”Ÿæˆkeypadding mask
        mask=(key==self.pad_idx).unsqueeze(1).unsqueeze(2)
        #è™½ç„¶key.shape=[batch,seq_k,d_model]ä½†æ˜¯åˆ¤æ–­key==self.pad_idxæ—¶ï¼Œ
        #æ¯”è¾ƒçš„æ—¶è¯ç´¢å¼•åºåˆ—ï¼Œä¸æ¯”è¾ƒd_modelï¼Œå³ [batch, seq_k]
        #æ‰€ä»¥maskç”Ÿæˆçš„æ˜¯[batch,seq_k]çš„true/falseå¼ é‡
        #ç».unsqueeze(1).unsqueeze(2)æ‰©å±•æˆ[batch,1,1,seq_k]
        mask=mask.expand(batch,1,seq_q,seq_k).to(device)
        #.expand()å¤åˆ¶ä¸º[batch,1,seq_q,seq_k]
        if is_triu_mask:#future maskï¼šä½œç”¨äºdecoderè®­ç»ƒæ—¶ï¼Œä¿è¯å½“å‰ä½ç½®ä¹‹åçš„è¯ä¸ä¼šè¢«è§‚æµ‹åˆ°
            dst_triu_mask=torch.triu(torch.ones(seq_q,seq_k,dtype=torch.bool),diagonal=1)
            #torch.ones(seq_q,seq_k,dtype=torch.bool)åˆ›å»ºä¸€ä¸ªboolå‹çŸ©é˜µçš„å…¨ä¸ºtrueçš„[seq_q,seq_k]çŸ©é˜µ
            #torch.triu(,diagonal=1) ; torch.tril(input, diagonal=0)  # è¿”å›ä¸‹ä¸‰è§’éƒ¨åˆ†
            # torch.triu()è¿”å›è¾“å…¥çŸ©é˜µçš„ â€‹â€‹ä¸Šä¸‰è§’éƒ¨åˆ†â€‹,å…¶ä½™ä½ç½®ç½®é›¶
            # diagonalå‚æ•°å†³å®š â€‹â€‹ä»å“ªæ¡å¯¹è§’çº¿å¼€å§‹ä¿ç•™å€¼(0:ä¸»å¯¹è§’çº¿ï¼›-1ï¼šä¸»å¯¹è§’çº¿ä¸‹ä¸€æ¡ï¼›1ï¼šä¸»å¯¹è§’çº¿ä¸Šä¸€æ¡)
            dst_triu_mask=dst_triu_mask.unsqueeze(0).unsqueeze(1).expand(batch,1,seq_q,seq_k).to(device)
            return mask|dst_triu_mask
            #decoderåˆå¹¶keyPadding maskå’Œfuture mask
        return mask

    def forward(self,src,dst):
        src_mask=self.generate_mask(src,src)
        encoder_out=self.encoder(src,src_mask)
        dst_mask=self.generate_mask(dst,dst,is_triu_mask=True)
        src_dst_mask=self.generate_mask(dst,src)
        #cross attentionä¸­çš„qæ¥è‡ªencoderï¼Œkæ¥è‡ªdecoderæ‰€ä»¥å…¶seq_lenä¸ä¸€å®šä¸€è‡´
        decoder_out=self.decoder(dst,encoder_out,dst_mask,src_dst_mask)
        out =self.out_linear(decoder_out)
        return out


    

if __name__=="__main__":
    #PositionEncoding(512,100)
    att = Transformer(100,120,0,6,8,512,1024)
    x=torch.randint(0,100,(5,64))
    y=torch.randint(0,120,(5,64))
    out=att(x,y)
    print(out.shape)
